## ML Conversations with JK

### Introduction 

Hi, welcome to our new reading group.  We will have 3-4 'conversations' with me (Justin) leading starting 2/10.  In the first talk, we will go right in and start with Support Vector Machines (SVMs).  I was inspired to start somewhere not too advanced but also not too basic so that everyone can find a place in the discussion. Naturally, we can tailor this as we progress.

For the SVM talk I'll include some pre-meeting reading as well as additional links and videos for you to watch.  To be clear, *you are not expected to complete all of the materials before the meeting*, though I recommend at least reading the primary document.  This helps the conversation to move relatively faster through the introduction and allows us time on the meatier parts: deeper understanding and application.  

Going forward, we'll pick together the direction we want to go and in the ideal setting, everyone will take a gander at leading one of our talks.  I'll include a list of topics below for us to pick from, but feel free to also suggest other materials for us to foray into. 



### Purpose 

Remember, our goal is simple: **grow as data scientists**.  This occurs as we challenge ourselves to read and present ML papers and methods to one another.  It also happens when we approach these methods in a more rigorous way, which has the added benefit of helping us prepare better for the more challenging interviews facing us.

On this note, putting one another on the spot should be par for course.  You are to be challenged, pushed and pulled in directions you are not comfortable with, all with the intention of becoming truly immersed in the method you are learning.

Successful reading groups elsewhere such as LinkedIn inevitably start applying their discussion topics and methods into their work.  It's a bit of a stretch now, but I have the hope that we can reach that stage within a feasible amount of time to really grow here.  On the side note, if this takes off I can probably get a few guest speakers to come out and give a few talks.  These will be people from my network but will mainly consist of engineers and scientists in the field as well as a few academics (whoever is nice enough to say yes).



### Suggested Topics

*(expect this list to grow over the next week)*

**Traditional Topics**

- Introduction to ML* Supervised learning:
  - Regression: Linear regression & regularization
  - Classification: Logistic regression, Naive Bayes, SVMs,
- Kernel trick & kernel methods (if there is time)
- Unsupervised learning: K-means and mixture models, PCA, embeddings
- Expectation Maximization (the EM algorithm).
- Graphical Models basics (at a very high level).
- What are PGMs, Markov chains, HMMs, MRFs, CRFs?
- ML for discrete data a.k.a. manifold learning, word embeddings, topic modeling (latent semantic indexing and Latent Dirichlet Allocation (LDA)), and decision trees and random forest (& GBDT)

**Related Topics**

- High dimensional sparse models and situations
- Causal inference
- Econometric methods
- Probability that matters (distributions, calculations, theory)



### Resources

*(expect this list to grow over the next week)*

**Textbooks & Background**

[Linear Algebra](http://www.cs.columbia.edu/~jebara/4771/tutorials/linear_algebra.pdf)

[Deep Learning (Goodfellow, Y. Bengio & Courville)](http://www.deeplearningbook.org/)

[Elements of Statistical Learning (Hastie & Tibshirani)](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)

[Reinforcement Learning (Sutton & Barto)](http://www.incompleteideas.net/book/RLbook2018.pdf)

[Convex Optimization (Boyd & Vandenberghe)](https://web.stanford.edu/~boyd/cvxbook/)

**Neural Nets & Backpropagation**

[The Perceptron (Rosenblatt, 1958)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)

[The Perceptron Tutorial](http://www.cs.columbia.edu/~jebara/4771/tutorials/perceptron.pdf)

Elements of Statistical Learning (Hastie, Tibshirani, & Friedman) - Section 4.5 & Chapter 11: Neural Networks

**Week 1 Reading: Support Vector Machines**

[Probabilistic ML (Murphy) - Introduction](https://www.cs.ubc.ca/~murphyk/MLbook/pml-intro-22may12.pdf)

[Support Vector Machines (Cortes & Vapnik, 1995)](http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf)

**Week 2 Reading: Ridge & Lasso Regressions**

[Reference] [Tibshirani's original paper on Lasso (1996)](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)

[Reference] [Hoerl's original paper on Ridge Regressions (1970)](https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf)

[An undergrad's understanding of model selection, page 8](https://www.whitman.edu/Documents/Academics/Mathematics/DeVine.pdf)

