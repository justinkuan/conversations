## ML Conversations with JK

This is the repository for all projects and resources related to ML Conversations reading group.

[Projects](#projects)

[Resources](#resources)

[Suggested Topics](#suggested-topics)

[Introduction to the reading group](introductions.md)

### Projects

*Multi-arm bandits* (adding soon)

### Resources

*(expect this list to grow over the next week)*

**Textbooks & Background**

[Linear Algebra](http://www.cs.columbia.edu/~jebara/4771/tutorials/linear_algebra.pdf)

[Deep Learning (Goodfellow, Y. Bengio & Courville)](http://www.deeplearningbook.org/)

[Elements of Statistical Learning (Hastie & Tibshirani)](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)

[Reinforcement Learning (Sutton & Barto)](http://www.incompleteideas.net/book/RLbook2018.pdf)

[Convex Optimization (Boyd & Vandenberghe)](https://web.stanford.edu/~boyd/cvxbook/)

**Neural Nets & Backpropagation**

[The Perceptron (Rosenblatt, 1958)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)

[The Perceptron Tutorial](http://www.cs.columbia.edu/~jebara/4771/tutorials/perceptron.pdf)

Elements of Statistical Learning (Hastie, Tibshirani, & Friedman) - Section 4.5 & Chapter 11: Neural Networks

**Week 1 Reading: Support Vector Machines**

[Probabilistic ML (Murphy) - Introduction](https://www.cs.ubc.ca/~murphyk/MLbook/pml-intro-22may12.pdf)

[Support Vector Machines (Cortes & Vapnik, 1995)](http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf)

**Week 2 Reading: Ridge & Lasso Regressions**

[Reference] [Tibshirani's original paper on Lasso (1996)](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)

[Reference] [Hoerl's original paper on Ridge Regressions (1970)](https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf)

[An undergrad's understanding of model selection, page 8](https://www.whitman.edu/Documents/Academics/Mathematics/DeVine.pdf)

### Suggested Topics

*(list is work-in-progress)*

- Introduction to ML* Supervised learning:
  - Regression: Linear regression & regularization
  - Classification: Logistic regression, Naive Bayes, SVMs,
- Kernel trick & kernel methods (if there is time)
- Unsupervised learning: K-means and mixture models, PCA, embeddings
- Multi-Arm Bandits

- Causal inference topics
- Econometric methods
- Probability that matters (distributions, calculations, theory)



